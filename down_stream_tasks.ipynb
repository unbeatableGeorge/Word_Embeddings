{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-27T13:41:37.045446Z",
     "start_time": "2025-08-27T13:36:48.211424Z"
    }
   },
   "source": [
    "# embkit_run.py\n",
    "# 依赖：pip install gensim scikit-learn datasets numpy scipy scikit-learn-intelex\n",
    "import os, json, numpy as np\n",
    "from typing import Dict, Tuple, List, Iterable\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datasets import load_dataset\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# =====================\n",
    "# A. 加载三种词向量\n",
    "# =====================\n",
    "def load_glove(win: int, dim: int, root: str = \"./models_glove\") -> KeyedVectors:\n",
    "    \"\"\"\n",
    "    加载你在 GloVe.ipynb 里保存的 .kv 文件，例如 glove_win8_dim200.kv\n",
    "    \"\"\"\n",
    "    path = os.path.join(root, f\"glove_win{win}_dim{dim}.kv\")\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"GloVe .kv not found: {path}\")\n",
    "    kv = KeyedVectors.load(path, mmap='r')\n",
    "    kv.fill_norms()\n",
    "    return kv\n",
    "\n",
    "def load_word2vec(win: int, dim: int, root: str = \"./models_word2vec\") -> KeyedVectors:\n",
    "    \"\"\"\n",
    "    可选：加载 Skip-gram（gensim Word2Vec .model）\n",
    "    \"\"\"\n",
    "    path = os.path.join(root, f\"word2vec_win{win}_dim{dim}.model\")\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Word2Vec .model not found: {path}\")\n",
    "    model = Word2Vec.load(path)\n",
    "    kv = model.wv\n",
    "    kv.fill_norms()\n",
    "    return kv\n",
    "\n",
    "def load_sppmi(win: int, dim: int, root: str = \"./models_sppmi\", id2word_path: str = \"id2word.json\",\n",
    "               mean_center: bool = False, l2_normalize: bool = True) -> KeyedVectors:\n",
    "    \"\"\"\n",
    "    加载 SPPMI：emb_win{win}_dim{dim}.npy + id2word.json -> KeyedVectors\n",
    "    \"\"\"\n",
    "    emb_path = os.path.join(root, f\"emb_win{win}_dim{dim}.npy\")\n",
    "    if not os.path.exists(emb_path):\n",
    "        raise FileNotFoundError(f\"SPPMI .npy not found: {emb_path}\")\n",
    "    if not os.path.exists(id2word_path):\n",
    "        raise FileNotFoundError(f\"id2word not found: {id2word_path}\")\n",
    "    emb = np.load(emb_path).astype(np.float32)\n",
    "\n",
    "    with open(id2word_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        id2word = json.load(f)\n",
    "    if isinstance(id2word, dict):\n",
    "        try:\n",
    "            words = [id2word[str(i)] for i in range(len(id2word))]\n",
    "        except KeyError:\n",
    "            words = [id2word[i] for i in range(len(id2word))]\n",
    "    elif isinstance(id2word, list):\n",
    "        words = id2word\n",
    "    else:\n",
    "        raise ValueError(\"id2word must be list or dict\")\n",
    "\n",
    "    if len(words) != emb.shape[0]:\n",
    "        words = words[:emb.shape[0]]\n",
    "        if len(words) != emb.shape[0]:\n",
    "            raise ValueError(f\"len(id2word) ({len(words)}) != emb rows ({emb.shape[0]})\")\n",
    "\n",
    "    X = emb.copy()\n",
    "    if mean_center:\n",
    "        X -= X.mean(0, keepdims=True)\n",
    "    if l2_normalize:\n",
    "        X /= (np.linalg.norm(X, axis=1, keepdims=True) + 1e-9)\n",
    "\n",
    "    kv = KeyedVectors(vector_size=X.shape[1])\n",
    "    kv.add_vectors(words, X)\n",
    "    kv.fill_norms()\n",
    "    return kv\n",
    "\n",
    "# =====================\n",
    "# B. 可选：ABTT 后处理（去掉前 r 个主成分）\n",
    "# =====================\n",
    "def abtt_inplace(kv: KeyedVectors, r: int = 2):\n",
    "    \"\"\"\n",
    "    All-but-the-Top：去掉前 r 个主成分（常见 r=1~3），能稳一点相似度/类比。\n",
    "    直接在 kv.vectors 上原地修改，改完需要重新 fill_norms()。\n",
    "    \"\"\"\n",
    "    if r <= 0:\n",
    "        return\n",
    "    X = kv.vectors\n",
    "    X = X - X.mean(0, keepdims=True)\n",
    "    # 直接用 SVD 取前 r 个主成分\n",
    "    U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    P = Vt[:r]  # (r, d)\n",
    "    X = X - X @ P.T @ P\n",
    "    kv.vectors[:] = X\n",
    "    kv.fill_norms()\n",
    "\n",
    "# =====================\n",
    "# C. 文本表示（句子/文档平均）\n",
    "# =====================\n",
    "def tokenize(s: str) -> List[str]:\n",
    "    # 简单 tokenizer：只保留纯字母的 token；可替换为更强的分词器\n",
    "    return [w for w in s.lower().split() if w.isalpha()]\n",
    "\n",
    "def sent_embedding_avg(kv: KeyedVectors, text: str, unk: str = None) -> np.ndarray:\n",
    "    toks = tokenize(text)\n",
    "    vecs = [kv[w] for w in toks if w in kv]\n",
    "    if not vecs and (unk is not None) and (unk in kv):\n",
    "        vecs = [kv[unk]]\n",
    "    if not vecs:\n",
    "        return np.zeros(kv.vector_size, dtype=np.float32)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "def batch_embed(kv: KeyedVectors, texts: Iterable[str]) -> np.ndarray:\n",
    "    return np.stack([sent_embedding_avg(kv, t) for t in texts], axis=0)\n",
    "\n",
    "# =====================\n",
    "# D1. 下游：AG News 文本分类（已修正切片）\n",
    "# =====================\n",
    "def evaluate_agnews(kv: KeyedVectors, train_size: int = 20000, test_size: int = 7600,\n",
    "                    C: float = 2.0, max_iter: int = 1000, random_state: int = 42) -> Dict[str, float]:\n",
    "    ds = load_dataset(\"ag_news\")\n",
    "\n",
    "    # 用 select 选行，再取列，避免“string indices must be integers”错误\n",
    "    train_ds = ds[\"train\"].select(range(min(train_size, len(ds[\"train\"]))))\n",
    "    test_ds  = ds[\"test\"].select(range(min(test_size,  len(ds[\"test\"]))))\n",
    "\n",
    "    X_train_texts = train_ds[\"text\"]   # List[str]\n",
    "    y_train = np.array(train_ds[\"label\"])\n",
    "    X_test_texts  = test_ds[\"text\"]\n",
    "    y_test = np.array(test_ds[\"label\"])\n",
    "\n",
    "    X_train = batch_embed(kv, X_train_texts)\n",
    "    X_test  = batch_embed(kv, X_test_texts)\n",
    "\n",
    "    # 逻辑回归做线性分类器\n",
    "    clf = LogisticRegression(\n",
    "        solver=\"saga\", penalty=\"l2\", C=C, max_iter=max_iter,\n",
    "        random_state=random_state, n_jobs=-1\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    return {\"acc\": float(acc), \"train_size\": int(len(y_train)), \"test_size\": int(len(y_test))}\n",
    "\n",
    "# =====================\n",
    "# D2. 下游：STS-B 语义相似度\n",
    "# =====================\n",
    "def evaluate_stsb(kv: KeyedVectors, split: str = \"validation\") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    GLUE STS-B 标签范围 [0,5]。我们直接用余弦与人类分数做皮尔逊/斯皮尔曼相关。\n",
    "    \"\"\"\n",
    "    ds = load_dataset(\"glue\", \"stsb\")[split]\n",
    "    s1 = [x[\"sentence1\"] for x in ds]\n",
    "    s2 = [x[\"sentence2\"] for x in ds]\n",
    "    y  = np.array([x[\"label\"] for x in ds], dtype=np.float32)\n",
    "\n",
    "    E1 = batch_embed(kv, s1)\n",
    "    E2 = batch_embed(kv, s2)\n",
    "\n",
    "    num = np.sum(E1 * E2, axis=1)\n",
    "    den = (np.linalg.norm(E1, axis=1) * np.linalg.norm(E2, axis=1) + 1e-9)\n",
    "    cos = num / den\n",
    "\n",
    "    pear = pearsonr(cos, y)[0]\n",
    "    spear = spearmanr(cos, y)[0]\n",
    "    return {\"pearson\": float(pear), \"spearman\": float(spear), \"n\": int(len(y))}\n",
    "\n",
    "def evaluate_yelp_polarity(kv, train_size=50000, test_size=5000, C=2.0, max_iter=1000, random_state=42):\n",
    "    ds = load_dataset(\"yelp_polarity\")\n",
    "    train_ds = ds[\"train\"].select(range(min(train_size, len(ds[\"train\"]))))\n",
    "    test_ds  = ds[\"test\"].select(range(min(test_size,  len(ds[\"test\"]))))\n",
    "\n",
    "    Xtr = batch_embed(kv, train_ds[\"text\"])\n",
    "    ytr = np.array(train_ds[\"label\"])\n",
    "    Xte = batch_embed(kv, test_ds[\"text\"])\n",
    "    yte = np.array(test_ds[\"label\"])\n",
    "\n",
    "    clf = LogisticRegression(solver=\"saga\", penalty=\"l2\", C=C, max_iter=max_iter,\n",
    "                             random_state=random_state, n_jobs=-1)\n",
    "    clf.fit(Xtr, ytr)\n",
    "    acc = accuracy_score(yte, clf.predict(Xte))\n",
    "    return {\"acc\": float(acc), \"train_size\": int(len(ytr)), \"test_size\": int(len(yte))}\n",
    "\n",
    "def _pair_features(U, V):\n",
    "    # U, V: (n, d)\n",
    "    return np.hstack([U, V, np.abs(U - V), U * V])\n",
    "\n",
    "def evaluate_qqp(kv, train_size=50000, valid_size=10000, C=2.0, max_iter=1000, random_state=42):\n",
    "    ds_tr = load_dataset(\"glue\", \"qqp\")[\"train\"].select(range(train_size))\n",
    "    ds_va = load_dataset(\"glue\", \"qqp\")[\"validation\"].select(range(valid_size))\n",
    "\n",
    "    Utr = batch_embed(kv, ds_tr[\"question1\"])\n",
    "    Vtr = batch_embed(kv, ds_tr[\"question2\"])\n",
    "    Xtr = _pair_features(Utr, Vtr)\n",
    "    ytr = np.array(ds_tr[\"label\"])\n",
    "\n",
    "    Uva = batch_embed(kv, ds_va[\"question1\"])\n",
    "    Vva = batch_embed(kv, ds_va[\"question2\"])\n",
    "    Xva = _pair_features(Uva, Vva)\n",
    "    yva = np.array(ds_va[\"label\"])\n",
    "\n",
    "    clf = LogisticRegression(solver=\"saga\", penalty=\"l2\", C=C, max_iter=max_iter,\n",
    "                             random_state=random_state, n_jobs=-1)\n",
    "    clf.fit(Xtr, ytr)\n",
    "    acc = accuracy_score(yva, clf.predict(Xva))\n",
    "    return {\"acc\": float(acc), \"train_size\": int(len(ytr)), \"valid_size\": int(len(yva))}\n",
    "\n",
    "\n",
    "def evaluate_trec6(kv, train_size=5452, test_size=500, C=2.0, max_iter=1000, random_state=42):\n",
    "    # HuggingFace: \"trec\" 数据集，coarse 6类标签在 \"label-coarse\"\n",
    "    ds = load_dataset(\"trec\")\n",
    "    train_ds = ds[\"train\"].select(range(min(train_size, len(ds[\"train\"]))))\n",
    "    test_ds  = ds[\"test\"].select(range(min(test_size,  len(ds[\"test\"]))))\n",
    "\n",
    "    Xtr = batch_embed(kv, train_ds[\"text\"])\n",
    "    ytr = np.array(train_ds[\"coarse_label\"]) if \"coarse_label\" in train_ds.column_names else np.array(train_ds[\"label-coarse\"])\n",
    "    Xte = batch_embed(kv, test_ds[\"text\"])\n",
    "    yte = np.array(test_ds[\"coarse_label\"]) if \"coarse_label\" in test_ds.column_names else np.array(test_ds[\"label-coarse\"])\n",
    "\n",
    "    clf = LogisticRegression(solver=\"saga\", penalty=\"l2\", C=C, max_iter=max_iter,\n",
    "                             random_state=random_state, n_jobs=-1)\n",
    "    clf.fit(Xtr, ytr)\n",
    "    acc = accuracy_score(yte, clf.predict(Xte))\n",
    "    return {\"acc\": float(acc), \"train_size\": int(len(ytr)), \"test_size\": int(len(yte))}\n",
    "\n",
    "# =====================\n",
    "# F. 便捷统一入口（按需使用）\n",
    "# =====================\n",
    "LOADERS = {\n",
    "    \"glove\": load_glove,\n",
    "    \"w2v\": load_word2vec,\n",
    "    \"sppmi\": load_sppmi,\n",
    "}\n",
    "\n",
    "def load_model(kind: str, win: int, dim: int, **kwargs) -> KeyedVectors:\n",
    "    if kind not in LOADERS:\n",
    "        raise ValueError(f\"Unknown kind={kind}, choose from {list(LOADERS)}\")\n",
    "    return LOADERS[kind](win, dim, **kwargs)\n",
    "\n",
    "# =====================\n",
    "# G. 示例主程序\n",
    "# =====================\n",
    "if __name__ == \"__main__\":\n",
    "    models, model_name = [], [\"w2v\", \"sppmi\", \"glove\"]\n",
    "    WIN, DIM = 4, 200\n",
    "    # word2vec\n",
    "    models.append(load_word2vec(WIN, DIM, root = \"./word2vec/models_word2vec\"))\n",
    "    # SPPMI\n",
    "    models.append(load_sppmi(WIN, DIM, root=\"./SPPMI/models_sppmi\"))\n",
    "    # glove\n",
    "    models.append(load_model(\"glove\", win=8, dim=200, root=\"./GloVe/models_glove\"))\n",
    "\n",
    "\n",
    "    for i, kv in enumerate(models):\n",
    "        print(model_name[i])\n",
    "        # ---- 2) 跑 AG News 文本分类 ----\n",
    "        res_cls = evaluate_agnews(kv, train_size=20000, test_size=7600)\n",
    "        print(\"[AG News]\", res_cls)\n",
    "\n",
    "        # ---- 3) 跑 STS-B 句子相似度（验证集）----\n",
    "        res_sts = evaluate_stsb(kv, split=\"validation\")\n",
    "        print(\"[STS-B]\", res_sts)\n",
    "\n",
    "        print(\"[Yelp Polarity]\",  evaluate_yelp_polarity(kv, train_size=50000, test_size=5000))\n",
    "        print(\"[QQP]\",            evaluate_qqp(kv, train_size=50000, valid_size=10000))\n",
    "        print(\"[TREC-6]\",         evaluate_trec6(kv))\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v\n",
      "[AG News] {'acc': 0.8503947368421053, 'train_size': 20000, 'test_size': 7600}\n",
      "[STS-B] {'pearson': 0.5030552262224939, 'spearman': 0.5305749224499638, 'n': 1500}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 560000/560000 [00:00<00:00, 1564162.30 examples/s]\n",
      "Generating test split: 100%|██████████| 38000/38000 [00:00<00:00, 1690713.40 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Yelp Polarity] {'acc': 0.8144, 'train_size': 50000, 'test_size': 5000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 363846/363846 [00:00<00:00, 5870647.67 examples/s]\n",
      "Generating validation split: 100%|██████████| 40430/40430 [00:00<00:00, 5076509.12 examples/s]\n",
      "Generating test split: 100%|██████████| 390965/390965 [00:00<00:00, 6609643.30 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QQP] {'acc': 0.7125, 'train_size': 50000, 'valid_size': 10000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 336k/336k [00:01<00:00, 230kB/s] \n",
      "Downloading data: 100%|██████████| 23.4k/23.4k [00:00<00:00, 26.1MB/s]\n",
      "Generating train split: 100%|██████████| 5452/5452 [00:00<00:00, 72436.54 examples/s]\n",
      "Generating test split: 100%|██████████| 500/500 [00:00<00:00, 69814.31 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TREC-6] {'acc': 0.85, 'train_size': 5452, 'test_size': 500}\n",
      "sppmi\n",
      "[AG News] {'acc': 0.8477631578947369, 'train_size': 20000, 'test_size': 7600}\n",
      "[STS-B] {'pearson': 0.37298143895901104, 'spearman': 0.45016584520149194, 'n': 1500}\n",
      "[Yelp Polarity] {'acc': 0.7788, 'train_size': 50000, 'test_size': 5000}\n",
      "[QQP] {'acc': 0.6931, 'train_size': 50000, 'valid_size': 10000}\n",
      "[TREC-6] {'acc': 0.706, 'train_size': 5452, 'test_size': 500}\n",
      "glove\n",
      "[AG News] {'acc': 0.8318421052631579, 'train_size': 20000, 'test_size': 7600}\n",
      "[STS-B] {'pearson': 0.3354405743842207, 'spearman': 0.41965032614010145, 'n': 1500}\n",
      "[Yelp Polarity] {'acc': 0.7964, 'train_size': 50000, 'test_size': 5000}\n",
      "[QQP] {'acc': 0.7024, 'train_size': 50000, 'valid_size': 10000}\n",
      "[TREC-6] {'acc': 0.762, 'train_size': 5452, 'test_size': 500}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "eb47de7143ee1ea3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
