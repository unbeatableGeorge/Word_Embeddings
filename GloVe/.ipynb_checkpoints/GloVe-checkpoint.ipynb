{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "703b1848c8d2b0ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T11:12:37.182499Z",
     "start_time": "2025-08-27T11:12:36.187131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "NNZ: 37294295\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch, numpy as np\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "# ---- 设备：优先用 CUDA ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "V = 50000\n",
    "win = window_size = 8\n",
    "d = 200\n",
    "\n",
    "save_dir = \"./models_glove\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "fname = os.path.join(save_dir, f\"CO_win{win}_dim{d}.npz\")\n",
    "\n",
    "# ---- 确保 CO 是 coo，并取出 (row, col, val) ----\n",
    "CO = load_npz(fname)  # 如果已经是 coo 可省略\n",
    "CO = CO.tocoo()\n",
    "\n",
    "rows_np = CO.row.astype(np.int64)\n",
    "cols_np = CO.col.astype(np.int64)\n",
    "vals_np = CO.data.astype(np.float32)\n",
    "\n",
    "# ---- 张量放到 GPU（最小更改：只搬 indices 和 values）----\n",
    "rows = torch.from_numpy(rows_np).to(device)\n",
    "cols = torch.from_numpy(cols_np).to(device)\n",
    "x_ij = torch.from_numpy(vals_np).to(device)      # 共现计数\n",
    "nnz = x_ij.numel()\n",
    "print(\"NNZ:\", nnz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a6618ac997633a07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T11:12:37.430582Z",
     "start_time": "2025-08-27T11:12:37.185688Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "xmax = 10      # GloVe 的标准超参，可按需调\n",
    "alpha = 0.75\n",
    "\n",
    "# ---- Embedding 参数（目标/上下文 + bias）----\n",
    "W  = nn.Embedding(V, d).to(device)  # target\n",
    "C  = nn.Embedding(V, d).to(device)  # context\n",
    "bW = nn.Embedding(V, 1).to(device)\n",
    "bC = nn.Embedding(V, 1).to(device)\n",
    "\n",
    "# 初始化（简单、稳定）\n",
    "for emb in [W, C]:\n",
    "    nn.init.uniform_(emb.weight, -0.5/d, 0.5/d)\n",
    "nn.init.zeros_(bW.weight); nn.init.zeros_(bC.weight)\n",
    "\n",
    "# ---- 优化器（改一行就行）----\n",
    "# Adam 在 CUDA 上支持最好；如果你用 sparse=True，可换 SparseAdam\n",
    "opt = torch.optim.Adam(\n",
    "    list(W.parameters()) + list(C.parameters()) + list(bW.parameters()) + list(bC.parameters()),\n",
    "    lr=2e-3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "369e9ee122bf407a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T11:21:02.725202Z",
     "start_time": "2025-08-27T11:12:37.442611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/20] avg loss = 0.250381\n",
      "[Epoch 2/20] avg loss = 0.132865\n",
      "[Epoch 3/20] avg loss = 0.112785\n",
      "[Epoch 4/20] avg loss = 0.098450\n",
      "[Epoch 5/20] avg loss = 0.085816\n",
      "[Epoch 6/20] avg loss = 0.074914\n",
      "[Epoch 7/20] avg loss = 0.066874\n",
      "[Epoch 8/20] avg loss = 0.061716\n",
      "[Epoch 9/20] avg loss = 0.058472\n",
      "[Epoch 10/20] avg loss = 0.056327\n",
      "[Epoch 11/20] avg loss = 0.054814\n",
      "[Epoch 12/20] avg loss = 0.053694\n",
      "[Epoch 13/20] avg loss = 0.052787\n",
      "[Epoch 14/20] avg loss = 0.052030\n",
      "[Epoch 15/20] avg loss = 0.051384\n",
      "[Epoch 16/20] avg loss = 0.050839\n",
      "[Epoch 17/20] avg loss = 0.050375\n",
      "[Epoch 18/20] avg loss = 0.049970\n",
      "[Epoch 19/20] avg loss = 0.049625\n",
      "[Epoch 20/20] avg loss = 0.049315\n"
     ]
    }
   ],
   "source": [
    "##import torch\n",
    "torch.set_float32_matmul_precision(\"high\")  # PyTorch 2.x 可选\n",
    "\n",
    "def glove_weight(x):\n",
    "    w = torch.pow(x / xmax, alpha)\n",
    "    w = torch.clamp(w, max=1.0)\n",
    "    return w\n",
    "\n",
    "# ---- 训练参数（可微调）----\n",
    "epochs = 20\n",
    "batch_size = 65536  # 如显存紧张可减小 (64k~256k)\n",
    "perm = torch.randperm(nnz, device=device)\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    total_loss = 0.0\n",
    "    # 每个 epoch 打乱一次\n",
    "    perm = torch.randperm(nnz, device=device)\n",
    "    for start in range(0, nnz, batch_size):\n",
    "        end = min(start + batch_size, nnz)\n",
    "        idx = perm[start:end]\n",
    "\n",
    "        i = rows[idx]\n",
    "        j = cols[idx]\n",
    "        x = x_ij[idx]\n",
    "\n",
    "        wi = W(i)          # (B, d)\n",
    "        cj = C(j)          # (B, d)\n",
    "        bi = bW(i).squeeze(-1)  # (B,)\n",
    "        bj = bC(j).squeeze(-1)  # (B,)\n",
    "\n",
    "        # 预测和目标\n",
    "        pred = (wi * cj).sum(dim=1) + bi + bj\n",
    "        logx = torch.log(x)\n",
    "\n",
    "        w = glove_weight(x)\n",
    "        loss = (w * (pred - logx)**2).mean()\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_loss += loss.item() * (end - start)\n",
    "\n",
    "    avg = total_loss / nnz\n",
    "    print(f\"[Epoch {ep}/{epochs}] avg loss = {avg:.6f}\")\n",
    "\n",
    "# window = 4, dim = 100, train_time = 3m 4s\n",
    "# window = 4, dim = 200, train_time = 5m 26s\n",
    "# window = 6, dim = 100, train_time = 4m 18s\n",
    "# window = 6, dim = 200, train_time = 6m 59s\n",
    "# window = 8, dim = 100, train_time = 5m 0s\n",
    "# window = 8, dim = 200, train_time = 8m 25s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "114848c8258a4dfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T11:21:05.473442Z",
     "start_time": "2025-08-27T11:21:02.744246Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVED] npy/txt/kv exported.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "with torch.no_grad():\n",
    "    final = W.weight + C.weight          # (V, d)\n",
    "    final = final.detach().cpu().numpy()\n",
    "    final = normalize(final)             # 行归一化，便于余弦相似\n",
    "\n",
    "# ---- 保存 .npy ----\n",
    "fname = os.path.join(save_dir, f\"glove_win{window_size}_dim{d}.npy\")\n",
    "np.save(fname, final)\n",
    "\n",
    "# ---- load .txt（word + 向量）----\n",
    "with open(\"id2word.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    id2word = json.load(f)\n",
    "\n",
    "fname = os.path.join(save_dir, f\"glove_win{window_size}_dim{d}.txt\")\n",
    "with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(V):\n",
    "        idx = str(i)\n",
    "        w = id2word[idx]\n",
    "        vec = \" \".join(f\"{x:.6f}\" for x in final[i])\n",
    "        f.write(f\"{w} {vec}\\n\")\n",
    "\n",
    "# ---- 保存为 Gensim KeyedVectors（便于 evaluate_word_pairs / most_similar）----\n",
    "from gensim.models import KeyedVectors\n",
    "kv = KeyedVectors(vector_size=d)\n",
    "kv.add_vectors([id2word[str(i)] for i in range(V)], final)\n",
    "\n",
    "fname = os.path.join(save_dir, f\"glove_win{window_size}_dim{d}.kv\")\n",
    "kv.save(fname)\n",
    "print(\"[SAVED] npy/txt/kv exported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6ed8a7a6262947db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T11:21:05.502325Z",
     "start_time": "2025-08-27T11:21:05.476750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (50000, 200)\n",
      "         the : [ 0.09024875  0.06032214 -0.05287563 -0.04998    -0.0284281   0.08411795\n",
      " -0.03706627  0.03375566]\n",
      "          of : [ 0.05926024  0.03618823 -0.09434661 -0.07992446 -0.03884495  0.04664911\n",
      " -0.05659388  0.03590697]\n",
      "         and : [-0.02759532  0.02639293 -0.0614111  -0.00533459 -0.03225466  0.01557078\n",
      " -0.01585256  0.02070977]\n",
      "\n",
      "Top-10 similar to 'king':\n",
      "            iii  0.6073\n",
      "            son  0.6000\n",
      "          queen  0.5920\n",
      "          henry  0.5607\n",
      "        kingdom  0.5564\n",
      "             ii  0.5522\n",
      "          kings  0.5482\n",
      "        himself  0.5398\n",
      "            vii  0.5368\n",
      "        charles  0.5266\n"
     ]
    }
   ],
   "source": [
    "# 形状\n",
    "print(\"Embedding shape:\", final.shape)   # (V, d)\n",
    "\n",
    "# 前 3 个词的前 8 维（示例）\n",
    "for i in range(3):\n",
    "    idx = str(i)\n",
    "    print(f\"{id2word[idx]:>12s} : {final[i, :8]}\")\n",
    "\n",
    "# 查相似词（用 kv or 直接 numpy）\n",
    "def most_similar_word(word, topn=10):\n",
    "    if word not in kv:\n",
    "        print(f\"'{word}' OOV.\")\n",
    "        return\n",
    "    for term, score in kv.most_similar(word, topn=topn):\n",
    "        print(f\"{term:>15s}  {score:.4f}\")\n",
    "\n",
    "print(\"\\nTop-10 similar to 'king':\")\n",
    "most_similar_word(\"king\", topn=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539708d4cca381be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T11:21:05.572806Z",
     "start_time": "2025-08-27T11:21:05.569284Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
